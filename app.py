from __future__ import annotations

import os
import json
import orjson
import streamlit as st

from prompts import (
	ANALYZER_SYSTEM_PROMPT,
	ANALYZER_USER_TEMPLATE,
	EDITOR_SYSTEM_PROMPT,
	EDITOR_USER_TEMPLATE,
)
from llm_client import chat_json, chat_text
from pdf_utils import extract_text_from_pdf

ANALYZER_MODEL = os.getenv("ANALYZER_MODEL", "gpt-4o-mini")
EDITOR_MODEL = os.getenv("EDITOR_MODEL", "gpt-4o")

st.set_page_config(page_title="–ù–µ–π—Ä–æ‚ÄëHR ‚Äî –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–¥–∞–∫—Ç—É—Ä–∞ —Ä–µ–∑—é–º–µ", layout="wide")

st.title("üéØ –ù–µ–π—Ä–æ‚ÄëHR ‚Äî –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–¥–∞–∫—Ç—É—Ä–∞ —Ä–µ–∑—é–º–µ")

with st.sidebar:
	st.header("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
	resume_pdf = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ä–µ–∑—é–º–µ", type=["pdf"])  # type: ignore
	resume_text_manual = st.text_area("–ò–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ", height=180)
	job_description = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏", height=180)
	analyzer_temp = st.slider("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä", 0.0, 0.5, 0.1, 0.1)
	editor_temp = st.slider("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –†–µ–¥–∞–∫—Ç–æ—Ä", 0.0, 0.6, 0.3, 0.1)


def load_resume_text() -> str:
	if resume_pdf is not None:
		# Save to temp and extract
		tmp_path = os.path.join(st.session_state.get("tmp_dir", "."), "_resume_tmp.pdf")
		with open(tmp_path, "wb") as f:
			f.write(resume_pdf.getbuffer())
		return extract_text_from_pdf(tmp_path)
	return resume_text_manual.strip()


def format_analysis_report(analysis_json: dict) -> str:
	"""–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç JSON-–æ—Ç—á—ë—Ç –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π Markdown"""
	report = []
	
	# –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
	if "overall_assessment" in analysis_json:
		report.append(f"### –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞\n{analysis_json['overall_assessment']}\n")
	
	# –¢–æ–ø –ø—Ä–æ–±–ª–µ–º
	if "top_issues" in analysis_json and analysis_json["top_issues"]:
		report.append("### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
		for i, issue in enumerate(analysis_json["top_issues"], 1):
			severity = issue.get("severity", "medium")
			severity_badge = {
				"high": "**–ö–†–ò–¢–ò–ß–ù–û**", 
				"medium": "**–í–ê–ñ–ù–û**", 
				"low": "**–ú–ï–õ–ö–û**"
			}.get(severity, "**–í–ê–ñ–ù–û**")
			
			report.append(f"#### {i}. {severity_badge}")
			report.append(f"**–ü—Ä–æ–±–ª–µ–º–∞:** {issue.get('issue', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞')}")
			report.append(f"**–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:** {issue.get('why', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
			report.append(f"**–†–µ—à–µ–Ω–∏–µ:** {issue.get('fix_suggestion', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n")
	
	# –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
	if "missing_data" in analysis_json and analysis_json["missing_data"]:
		report.append("## –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
		for missing in analysis_json["missing_data"]:
			field_name = missing.get('field', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –ø–æ–ª–µ')
			field_display = {
				"metric": "–ú–µ—Ç—Ä–∏–∫–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
				"dates": "–î–∞—Ç—ã —Ä–∞–±–æ—Ç—ã", 
				"location": "–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ",
				"education": "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ",
				"contact": "–ö–æ–Ω—Ç–∞–∫—Ç—ã",
				"skills": "–ù–∞–≤—ã–∫–∏"
			}.get(field_name, f"{field_name}")
			report.append(f"- **{field_display}:** {missing.get('note', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
		report.append("")
	
	# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
	if "keywords_match" in analysis_json:
		keywords = analysis_json["keywords_match"]
		report.append("### –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
		if keywords.get("found_in_resume"):
			report.append(f"**–ù–∞–π–¥–µ–Ω–æ –≤ —Ä–µ–∑—é–º–µ:** {', '.join(keywords['found_in_resume'])}")
		if keywords.get("missing"):
			report.append(f"**–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç:** {', '.join(keywords['missing'])}")
		report.append("")
	
	# –†–∏—Å–∫–∏
	if "risks" in analysis_json and analysis_json["risks"]:
		report.append("### –†–∏—Å–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
		for risk in analysis_json["risks"]:
			report.append(f"- {risk}")
		report.append("")
	
	# –í–æ–ø—Ä–æ—Å—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—É
	if "candidate_questions" in analysis_json and analysis_json["candidate_questions"]:
		report.append("### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è")
		for i, question in enumerate(analysis_json["candidate_questions"], 1):
			report.append(f"{i}. {question}")
		report.append("")
	
	# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
	if "priority_fix_list" in analysis_json and analysis_json["priority_fix_list"]:
		report.append("### –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π")
		for i, fix in enumerate(analysis_json["priority_fix_list"], 1):
			report.append(f"**{i}.** {fix}")
		report.append("")
	
	return "\n".join(report)


st.header("üîπ 1) –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
	resume_text = load_resume_text()
	if not resume_text:
		st.warning("–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∑—é–º–µ (PDF –∏–ª–∏ —Ç–µ–∫—Å—Ç)")
	else:
		user_prompt = ANALYZER_USER_TEMPLATE.format(
			resume_text=resume_text,
			job_description=job_description or "",
		)
		messages = [
			{"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
			{"role": "user", "content": user_prompt},
		]
		with st.spinner("–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—é–º–µ‚Ä¶"):
			try:
				analysis_json = chat_json(
					messages=messages,
					model=ANALYZER_MODEL,
					temperature=float(analyzer_temp),
				)
				st.session_state["analysis_json"] = analysis_json
				st.success("–ì–æ—Ç–æ–≤–æ: –æ—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
			except Exception as e:
				st.error(f"–û—à–∏–±–∫–∞ LLM: {e}")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
if "analysis_json" in st.session_state:
	analysis_json = st.session_state["analysis_json"]
	
	# –°–æ–∑–¥–∞—ë–º —Ç–∞–±—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
	tab1, tab2 = st.tabs(["–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ", "JSON –¥–∞–Ω–Ω—ã–µ"])
	
	with tab1:
		st.markdown(format_analysis_report(analysis_json))
	
	with tab2:
		st.code(orjson.dumps(analysis_json, option=orjson.OPT_INDENT_2).decode(), language="json")


st.header("üîπ 2) –†–µ–¥–∞–∫—Ç–æ—Ä")
if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ"):
	resume_text = load_resume_text()
	if not resume_text:
		st.warning("–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∑—é–º–µ (PDF –∏–ª–∏ —Ç–µ–∫—Å—Ç)")
	else:
		if "analysis_json" not in st.session_state:
			st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ‚Äî –µ–≥–æ –≤—ã–≤–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –†–µ–¥–∞–∫—Ç–æ—Ä–æ–º")
		analysis_json_str = orjson.dumps(st.session_state.get("analysis_json", {})).decode()
		user_prompt = EDITOR_USER_TEMPLATE.format(
			analyzer_json=analysis_json_str,
			resume_text=resume_text,
			job_description=job_description or "",
		)
		messages = [
			{"role": "system", "content": EDITOR_SYSTEM_PROMPT},
			{"role": "user", "content": user_prompt},
		]
		with st.spinner("–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—é–º–µ‚Ä¶"):
			try:
				editor_output = chat_text(
					messages=messages,
					model=EDITOR_MODEL,
					temperature=float(editor_temp),
				)
				st.session_state["editor_output"] = editor_output
				st.success("–ì–æ—Ç–æ–≤–æ: —Ä–µ–∑—é–º–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
			except Exception as e:
				st.error(f"–û—à–∏–±–∫–∞ LLM: {e}")

if "editor_output" in st.session_state:
	st.subheader("–ò—Ç–æ–≥ (Markdown —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏)")
	st.markdown(st.session_state["editor_output"])  # Editor –≤—ã–≤–æ–¥–∏—Ç –ú–∞—Ä–∫–¥–∞—É–Ω –∏ —Å–ø–∏—Å–∫–∏

st.divider()

