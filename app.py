from __future__ import annotations

import os
import json
import orjson
import streamlit as st

from prompts import (
	ANALYZER_SYSTEM_PROMPT,
	ANALYZER_USER_TEMPLATE,
	EDITOR_SYSTEM_PROMPT,
	EDITOR_USER_TEMPLATE,
)
from llm_client import chat_json, chat_text
from pdf_utils import extract_text_from_pdf

ANALYZER_MODEL = os.getenv("ANALYZER_MODEL", "gpt-4o")
EDITOR_MODEL = os.getenv("EDITOR_MODEL", "gpt-4o")

st.set_page_config(page_title="–ù–µ–π—Ä–æ‚ÄëHR ‚Äî –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–¥–∞–∫—Ç—É—Ä–∞ —Ä–µ–∑—é–º–µ", layout="wide")
# Widen sidebar
st.markdown(
    """
    <style>
    [data-testid="stSidebar"] {width: 500px;}
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("üéØ –ù–µ–π—Ä–æ‚ÄëHR ‚Äî –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–¥–∞–∫—Ç—É—Ä–∞ —Ä–µ–∑—é–º–µ")

with st.sidebar:
	st.header("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
	resume_pdf = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ä–µ–∑—é–º–µ", type=["pdf"])  # type: ignore
	job_description = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏", height=450)


def load_resume_text() -> str:
    if resume_pdf is not None:
        # Save to temp and extract
        tmp_path = os.path.join(st.session_state.get("tmp_dir", "."), "_resume_tmp.pdf")
        with open(tmp_path, "wb") as f:
            f.write(resume_pdf.getbuffer())
        return extract_text_from_pdf(tmp_path)
    return ""


def _is_job_description_empty(text: str) -> bool:
    """Check JD emptiness per system rules: empty string, <20 words, or equals to N/A/none."""
    if not text or not text.strip():
        return True
    lowered = text.strip().lower()
    if lowered in {"n/a", "na", "none"}:
        return True
    # Count words conservatively
    words = [w for w in lowered.replace("\n", " ").split(" ") if w]
    return len(words) < 20


def _is_valid_analysis(analysis_json: dict) -> bool:
    """Minimal schema check for analyzer JSON to avoid UI errors."""
    if not isinstance(analysis_json, dict):
        return False
    required_top = [
        "overall_assessment",
        "clarity_assessment",
        "completeness_check",
        "volume_assessment",
        "keywords_match",
        "top_issues",
        "priority_fix_list",
    ]
    for key in required_top:
        if key not in analysis_json:
            return False
    return True


def format_analysis_report(analysis_json: dict) -> str:
	"""–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç JSON-–æ—Ç—á—ë—Ç –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π Markdown"""
	report = []
	
	# –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
	if "overall_assessment" in analysis_json:
		report.append(f"### –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞\n{analysis_json['overall_assessment']}\n")
	
	# –û—Ü–µ–Ω–∫–∞ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏
	if "clarity_assessment" in analysis_json:
		clarity = analysis_json["clarity_assessment"]
		rating = clarity.get("rating", "medium")
		report.append("### –û—Ü–µ–Ω–∫–∞ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏")
		report.append(f"**–†–µ–π—Ç–∏–Ω–≥:** {rating.upper()}")
		report.append(f"**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:** {clarity.get('why', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
		if clarity.get("suggestion"):
			report.append(f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {clarity['suggestion']}")
		report.append("")
	
	# –û—Ü–µ–Ω–∫–∞ –æ–±—ä–µ–º–∞
	if "volume_assessment" in analysis_json:
		volume = analysis_json["volume_assessment"]
		report.append("### –û—Ü–µ–Ω–∫–∞ –æ–±—ä–µ–º–∞")
		# –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∫–∞–∫ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –Ω–µ —Å–ª–∏–≤–∞–ª–æ—Å—å –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
		if volume.get("estimated_words"):
			report.append(f"- **–°–ª–æ–≤:** {volume['estimated_words']}")
		if volume.get("estimated_pages"):
			report.append(f"- **–°—Ç—Ä–∞–Ω–∏—Ü:** {volume['estimated_pages']}")
		if volume.get("relative_to_average"):
			report.append(f"- **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ:** {volume['relative_to_average']}")
		if volume.get("relative_to_golden_standard"):
			report.append(f"- **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —ç—Ç–∞–ª–æ–Ω–∞:** {volume['relative_to_golden_standard']}")
		if volume.get("why"):
			report.append(f"- **–ü–æ—á–µ–º—É:** {volume['why']}")
		if volume.get("suggestion"):
			report.append(f"- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {volume['suggestion']}")
		report.append("")
	
	# –¢–æ–ø –ø—Ä–æ–±–ª–µ–º
	if "top_issues" in analysis_json and analysis_json["top_issues"]:
		report.append("### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
		for i, issue in enumerate(analysis_json["top_issues"], 1):
			severity = issue.get("severity", "medium")
			severity_badge = {
				"high": "**–ö–†–ò–¢–ò–ß–ù–û**", 
				"medium": "**–í–ê–ñ–ù–û**", 
				"low": "**–ú–ï–õ–ö–û**"
			}.get(severity, "**–í–ê–ñ–ù–û**")
			
			report.append(f"#### {i}. {severity_badge}")
			report.append(f"**–ü—Ä–æ–±–ª–µ–º–∞:** {issue.get('issue', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞')}")
			report.append(f"**–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:** {issue.get('why', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
			report.append(f"**–†–µ—à–µ–Ω–∏–µ:** {issue.get('fix_suggestion', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n")
	
	# –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
	if "completeness_check" in analysis_json and analysis_json["completeness_check"]:
		report.append("## –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
		for missing in analysis_json["completeness_check"]:
			field_name = missing.get('field', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –ø–æ–ª–µ')
			status = missing.get('status', 'missing')
			field_display = {
				"contacts": "–ö–æ–Ω—Ç–∞–∫—Ç—ã",
				"role": "–¶–µ–ª–µ–≤–∞—è –¥–æ–ª–∂–Ω–æ—Å—Ç—å",
				"seniority": "–£—Ä–æ–≤–µ–Ω—å",
				"dates": "–î–∞—Ç—ã —Ä–∞–±–æ—Ç—ã", 
				"companies": "–ö–æ–º–ø–∞–Ω–∏–∏",
				"responsibilities": "–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏",
				"achievements_metrics": "–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏",
				"stack_tools": "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
				"education": "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ",
				"languages": "–Ø–∑—ã–∫–∏",
				"location": "–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ",
				"links": "–°—Å—ã–ª–∫–∏"
			}.get(field_name, f"{field_name}")
			
			status_display = {
				"present": "‚úÖ",
				"partial": "‚ö†Ô∏è",
				"missing": "‚ùå"
			}.get(status, "‚ùì")
			
			report.append(f"- **{field_display}:** {status_display} {missing.get('note', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
		report.append("")
	
	# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
	if "keywords_match" in analysis_json:
		keywords = analysis_json["keywords_match"]
		report.append("### –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
		
		# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ JD (–µ—Å–ª–∏ –µ—Å—Ç—å)
		if keywords.get("from_jd"):
			report.append(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏:** {', '.join(keywords['from_jd'])}")
		
		# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
		if keywords.get("found_exact"):
			report.append(f"**–ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ:** {', '.join(keywords['found_exact'])}")
		
		# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
		if keywords.get("found_fuzzy"):
			report.append(f"**–ù–∞–π–¥–µ–Ω–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ:** {', '.join(keywords['found_fuzzy'])}")
		
		# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
		if keywords.get("missing"):
			report.append(f"**–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç:** {', '.join(keywords['missing'])}")
		
		# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è
		if keywords.get("coverage_percent") is not None:
			coverage = keywords['coverage_percent']
			coverage_emoji = "üü¢" if coverage >= 80 else "üü°" if coverage >= 60 else "üî¥"
			report.append(f"**–ü–æ–∫—Ä—ã—Ç–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:** {coverage_emoji} {coverage}%")
		
		report.append("")
	
	# –í–æ–ø—Ä–æ—Å—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—É
	if "candidate_questions" in analysis_json and analysis_json["candidate_questions"]:
		report.append("### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è")
		for i, question in enumerate(analysis_json["candidate_questions"], 1):
			report.append(f"{i}. {question}")
		report.append("")
	
	# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
	if "priority_fix_list" in analysis_json and analysis_json["priority_fix_list"]:
		report.append("### –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π")
		for i, fix in enumerate(analysis_json["priority_fix_list"], 1):
			report.append(f"**{i}.** {fix}")
		report.append("")
	
	return "\n".join(report)


st.header("üîπ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
    resume_text = load_resume_text()
    if not resume_text:
        st.warning("–¢—Ä–µ–±—É–µ—Ç—Å—è PDF —Ä–µ–∑—é–º–µ")
    else:
        # Normalize JD: if too short/missing, force empty to trigger generic analysis scenario
        jd_for_model = "" if _is_job_description_empty(job_description or "") else (job_description or "")
        user_prompt = ANALYZER_USER_TEMPLATE.format(
            resume_text=resume_text,
            job_description=jd_for_model,
        )
        messages = [
            {"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]
        with st.spinner("–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—é–º–µ‚Ä¶"):
            try:
                analysis_json = chat_json(
                    messages=messages,
                    model=ANALYZER_MODEL,
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                if isinstance(analysis_json, dict) and "error" in analysis_json:
                    reason = str(analysis_json.get("reason", "")).lower()
                    # Fallback: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ—à–∏–±–ª–∞—Å—å –∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ/–∫–æ—Ä–æ—Ç–∫–æ–≥–æ JD ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏–º –≤ —Ä–µ–∂–∏–º–µ –æ–±—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    if any(k in reason for k in ["job description", "vacancy", "–æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏"]) and any(
                        k in reason for k in ["missing", "short", "–∫–æ—Ä–æ—Ç", "–Ω–µ—Ç", "–ø—É—Å—Ç"]
                    ):
                        fallback_prompt = ANALYZER_USER_TEMPLATE.format(
                            resume_text=resume_text,
                            job_description="",
                        )
                        fallback_messages = [
                            {"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
                            {"role": "user", "content": "–í–ù–ò–ú–ê–ù–ò–ï: –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –í—ã–ø–æ–ª–Ω—è–π –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑, –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–π –æ—à–∏–±–∫—É."},
                            {"role": "user", "content": fallback_prompt},
                        ]
                        try:
                            analysis_json_fb = chat_json(
                                messages=fallback_messages,
                                model=ANALYZER_MODEL,
                            )
                            # –ï—Å–ª–∏ –≤–µ—Ä–Ω—É–ª—Å—è –æ–±—ä–µ–∫—Ç-–æ—à–∏–±–∫–∞ –ò–õ–ò JSON –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–≥—É—é —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                            if (isinstance(analysis_json_fb, dict) and "error" in analysis_json_fb) or not _is_valid_analysis(analysis_json_fb):
                                strict_prompt = ANALYZER_USER_TEMPLATE.format(
                                    resume_text=resume_text,
                                    job_description="",
                                )
                                strict_messages = [
                                    {"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
                                    {"role": "user", "content": "–°–¢–†–û–ì–ò–ô –†–ï–ñ–ò–ú: –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON —Å—Ç—Ä–æ–≥–æ –ø–æ JSON Schema –±–µ–∑ –æ—à–∏–±–æ–∫ –∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."},
                                    {"role": "user", "content": strict_prompt},
                                ]
                                analysis_json_fb2 = chat_json(
                                    messages=strict_messages,
                                    model=ANALYZER_MODEL,
                                )
                                if isinstance(analysis_json_fb2, dict) and "error" not in analysis_json_fb2 and _is_valid_analysis(analysis_json_fb2):
                                    st.session_state["analysis_json"] = analysis_json_fb2
                                    st.info("–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ.")
                                    st.success("–ì–æ—Ç–æ–≤–æ: –æ—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
                                    # –ó–∞–≤–µ—Ä—à–∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫—É
                                    # (–Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å return)
                                # –ï—Å–ª–∏ —Å–Ω–æ–≤–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî –ø–æ–∫–∞–∂–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –æ—à–∏–±–∫—É
                                st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {analysis_json.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤–µ—Ç–∫–∏ —Å—Ç—Ä–æ–≥–æ–π —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                            if isinstance(analysis_json_fb, dict) and "error" not in analysis_json_fb:
                                st.session_state["analysis_json"] = analysis_json_fb
                                if _is_job_description_empty(job_description or ""):
                                    st.info("–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ.")
                                else:
                                    st.info("–ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ.")
                                st.success("–ì–æ—Ç–æ–≤–æ: –æ—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
                            else:
                                st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {analysis_json.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                        except Exception as _:
                            st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {analysis_json.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                    else:
                        st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {analysis_json.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                else:
                    # –ï—Å–ª–∏ –≤–µ—Ä–Ω—É–ª—Å—è JSON –±–µ–∑ –∫–ª—é—á–∞ error, –Ω–æ –æ–Ω –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ö–µ–º–µ ‚Äî –ø–æ–ø—Ä–æ—Å–∏–º —Å—Ç—Ä–æ–≥—É—é —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                    if not _is_valid_analysis(analysis_json):
                        strict_prompt = ANALYZER_USER_TEMPLATE.format(
                            resume_text=resume_text,
                            job_description=jd_for_model,
                        )
                        strict_messages = [
                            {"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
                            {"role": "user", "content": "–°–¢–†–û–ì–ò–ô –†–ï–ñ–ò–ú: –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON —Å—Ç—Ä–æ–≥–æ –ø–æ JSON Schema –±–µ–∑ –æ—à–∏–±–æ–∫ –∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."},
                            {"role": "user", "content": strict_prompt},
                        ]
                        try:
                            analysis_json2 = chat_json(
                                messages=strict_messages,
                                model=ANALYZER_MODEL,
                            )
                            if isinstance(analysis_json2, dict) and "error" not in analysis_json2 and _is_valid_analysis(analysis_json2):
                                st.session_state["analysis_json"] = analysis_json2
                                st.success("–ì–æ—Ç–æ–≤–æ: –æ—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
                            else:
                                st.error("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç JSON Schema")
                        except Exception as _:
                            st.error("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç JSON Schema")
                    else:
                        st.session_state["analysis_json"] = analysis_json
                        st.success("–ì–æ—Ç–æ–≤–æ: –æ—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ LLM: {e}")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç)
if "analysis_json" in st.session_state:
    analysis_json = st.session_state["analysis_json"]
    st.markdown(format_analysis_report(analysis_json))


st.header("üîπ –†–µ–¥–∞–∫—Ç–æ—Ä")
# Version selector for editor stage (Russian labels)
col_v1, col_v2 = st.columns([1, 3])
with col_v1:
    resume_version_label = st.radio("–í–µ—Ä—Å–∏—è —Ä–µ–∑—é–º–µ", options=["–ö–æ—Ä–æ—Ç–∫–∞—è", "–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è"], index=0)
resume_version = "concise" if resume_version_label == "–ö–æ—Ä–æ—Ç–∫–∞—è" else "full"

if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ"):
    resume_text = load_resume_text()
    if not resume_text:
        st.warning("–¢—Ä–µ–±—É–µ—Ç—Å—è PDF —Ä–µ–∑—é–º–µ")
    else:
        if "analysis_json" not in st.session_state:
            st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ‚Äî –µ–≥–æ –≤—ã–≤–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –†–µ–¥–∞–∫—Ç–æ—Ä–æ–º")
        analysis_json_str = orjson.dumps(st.session_state.get("analysis_json", {})).decode()
        user_prompt = EDITOR_USER_TEMPLATE.format(
            analyzer_json=analysis_json_str,
            resume_text=resume_text,
            job_description=job_description or "",
            resume_version=resume_version,
        )
        messages = [
            {"role": "system", "content": EDITOR_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]
        with st.spinner("–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—é–º–µ‚Ä¶"):
            try:
                editor_output = chat_text(
                    messages=messages,
                    model=EDITOR_MODEL,
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞
                if editor_output.strip().startswith('{"error":'):
                    try:
                        error_json = orjson.loads(editor_output)
                        st.error(f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞: {error_json.get('details', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                    except:
                        st.error(f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞: {editor_output}")
                else:
                    st.session_state["editor_output"] = editor_output
                    st.success("–ì–æ—Ç–æ–≤–æ: —Ä–µ–∑—é–º–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ LLM: {e}")

if "editor_output" in st.session_state:
	st.subheader("–ò—Ç–æ–≥ (Markdown —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏)")
	st.markdown(st.session_state["editor_output"])  # Editor –≤—ã–≤–æ–¥–∏—Ç –ú–∞—Ä–∫–¥–∞—É–Ω –∏ —Å–ø–∏—Å–∫–∏

st.divider()

st.header("üîπ –û—Ü–µ–Ω–∫–∞ –∑–∞—Ä–ø–ª–∞—Ç–Ω–æ–π –≤–∏–ª–∫–∏ (RUB/–º–µ—Å)")
with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å/—Å–∫—Ä—ã—Ç—å –æ—Ü–µ–Ω–∫—É –∑–∞—Ä–ø–ª–∞—Ç—ã"):
	try:
		from salary_estimator import estimate_salary_from_resume
		_resume_text = load_resume_text()
		if not _resume_text:
			st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ä–µ–∑—é–º–µ, —á—Ç–æ–±—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏—Ç—å –≤–∏–ª–∫—É –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ä–æ–ª–∏.")
		else:
			result = estimate_salary_from_resume(
				resume_text=_resume_text,
				job_description=job_description or None,
				model=ANALYZER_MODEL,
				temperature=0.1,
			)
			st.subheader("–ü–æ–¥—Ö–æ–¥—è—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏")
			roles = result.get("roles", []) or []
			if roles:
				for r in roles:
					title = r.get("title", "‚Äî")
					dirn = r.get("direction", "‚Äî")
					sen = r.get("seniority") or "‚Äî"
					reason = r.get("fit_reason", "")
					st.markdown(f"- **{title}** ‚Äî {dirn} ({sen})  ")
					if reason:
						st.caption(reason)
			st.subheader("–û—Ü–µ–Ω–∫–∞ —Ä—ã–Ω–æ—á–Ω–æ–π –≤–∏–ª–∫–∏ (RUB/–º–µ—Å)")
			est_overall = result.get("estimate_rub_month", {})
			if est_overall:
				st.markdown(f"**–ò—Ç–æ–≥–æ:** {est_overall.get('min', '‚Äî')} ‚Äî {est_overall.get('max', '‚Äî')} (–º–µ–¥–∏–∞–Ω–∞: {est_overall.get('median', '‚Äî')})")
			ranges = result.get("ranges_per_role", []) or []
			if ranges:
				with st.expander("–î–∏–∞–ø–∞–∑–æ–Ω—ã –ø–æ —Ä–æ–ª—è–º"):
					for rr in ranges:
						st.write(f"- {rr.get('title', '‚Äî')}: {rr.get('min', '‚Äî')} ‚Äî {rr.get('max', '‚Äî')} (–º–µ–¥–∏–∞–Ω–∞: {rr.get('median', '‚Äî')})")
			conf = result.get("confidence", "‚Äî")
			st.markdown(f"**–î–æ–≤–µ—Ä–∏–µ:** {conf}")
			notes = result.get("notes", "")
			if notes:
				st.markdown(f"**–ü—Ä–∏–º–µ—á–∞–Ω–∏—è:** {notes}")
			sources = result.get("sources", [])
			if sources:
				with st.expander("–ò—Å—Ç–æ—á–Ω–∏–∫–∏"):
					for s in sources:
						st.write(f"- {s}")
			assumptions = result.get("assumptions", [])
			if assumptions:
				with st.expander("–î–æ–ø—É—â–µ–Ω–∏—è"):
					for a in assumptions:
						st.write(f"- {a}")
	except Exception as e:
		st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∑–∞—Ä–ø–ª–∞—Ç—ã: {e}")
